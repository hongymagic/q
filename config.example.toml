# q configuration file
# Copy to ~/.config/q/config.toml or run `q config init`
#
# Config resolution order (later overrides earlier):
#   1. XDG config: ~/.config/q/config.toml (or $XDG_CONFIG_HOME/q/config.toml)
#   2. Project config: ./config.toml in current directory
#   3. Environment variables: Q_PROVIDER, Q_MODEL

[default]
provider = "anthropic"
model = "claude-sonnet-4-20250514"

[providers.anthropic]
type = "anthropic"
api_key_env = "ANTHROPIC_API_KEY"

[providers.openai]
type = "openai"
api_key_env = "OPENAI_API_KEY"

# OpenAI-compatible provider (e.g., LM Studio, LocalAI)
# [providers.local]
# type = "openai_compatible"
# base_url = "http://localhost:1234/v1"
# api_key_env = "LOCAL_API_KEY"

# Portkey Gateway - Internal/Self-hosted deployment
# Use type = "portkey" for first-class Portkey support
# [providers.portkey_internal]
# type = "portkey"
# base_url = "https://your-portkey-gateway.internal/v1"
# provider_slug = "@your-org/bedrock-provider"  # Maps to x-portkey-provider header
# api_key_env = "PORTKEY_API_KEY"               # Maps to x-portkey-api-key header (optional)
# provider_api_key_env = "PROVIDER_API_KEY"     # Maps to Authorization header (optional)
# headers = { "x-custom" = "${CUSTOM_VALUE}" }  # Additional headers (optional)

# Portkey Cloud (public API)
# [providers.portkey]
# type = "portkey"
# base_url = "https://api.portkey.ai/v1"
# provider_slug = "openai"
# api_key_env = "PORTKEY_API_KEY"

# Ollama (local models)
# [providers.ollama]
# type = "ollama"
# base_url = "http://localhost:11434"
